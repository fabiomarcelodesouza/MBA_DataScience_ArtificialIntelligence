claims %>% ggplot() + geom_point(aes(x = sexoc, y = cmsinistro))
claims %>% ggplot() + geom_boxplot(aes(x = sexoc, y = cmsinistro))
#Analise por area
claims %>% ggplot() + geom_point(aes(x = areac, y = cmsinistro))
claims %>% ggplot() + geom_boxplot(aes(x = areac, y = cmsinistro))
dummies_vars = dummyVars(~tipov + sexoc + areac, data = claims)
dummies_dados <- predict(dummies_vars, newdata = claims)
claims_dummies = cbind(claims, dummies_dados)
claims_dummies <- dplyr::select(claims_dummies, -c(tipov, sexoc, areac))
#matriz de correlação
correlacao_dummies <- cor(claims_dummies)
ggplot(data = reshape2::melt(correlacao_dummies)) +
geom_tile(aes(Var1, Var2, fill = value)) +
scale_fill_gradient(low = "blue", high = "white") +
theme_minimal() +
labs(title = "Mapa de Correlação Linear") +
theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))
as.data.frame(correlacao_dummies) %>% arrange(cmsinistro)
correlacao_forte <- correlacao_dummies[correlacao_dummies[,7] > 0.9, ]
#expos
#areacE
#csinistros
ggplot(data = reshape2::melt(correlacao_forte)) +
geom_tile(aes(Var1, Var2, fill = value)) +
scale_fill_gradient(low = "blue", high = "white") +
theme_minimal() +
labs(title = "Mapa de Correlação Linear") +
theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))
head(claims_dummies)
claims_features <- claims_dummies[c(1, 25, 4, 7)]
claims_features %>% ggplot() + geom_boxplot(aes(y = cmsinistro))
ggplot(data = claims_features, aes(y = cmsinistro)) +
geom_boxplot(fill = "lightblue") +
labs(title = "Boxplot da Sétima Coluna", y = "Valores")
# Calcular o primeiro quartil (Q1) e o terceiro quartil (Q3)
q1 <- quantile(claims_features$cmsinistro, probs = 0.25)
q3 <- quantile(claims_features$cmsinistro, 0.75)
# Calcular o intervalo interquartil (IQR)
iqr <- q3 - q1
# Definir os limites para identificar outliers
limite_inferior <- q1 - 1.5 * iqr
limite_superior <- q3 + 1.5 * iqr
# Remover outliers com base nos limites
claims_sem_outliers <- subset(claims_features, cmsinistro >= limite_superior)
# Definir a proporção desejada de treinamento e teste (por exemplo, 80% de treinamento, 20% de teste)
proporcao_treinamento <- 0.8
# Calcular o tamanho dos conjuntos de treinamento e teste com base na proporção
tamanho_treinamento <- round(nrow(claims_sem_outliers) * proporcao_treinamento)
tamanho_teste <- nrow(claims_sem_outliers) - tamanho_treinamento
# Criar índices aleatórios para amostrar os dados
indices_amostra <- sample(1:nrow(claims_sem_outliers))
# Dividir o dataframe em conjuntos de treinamento e teste
treino_regressao <- claims_sem_outliers[indices_amostra[1:tamanho_treinamento], ]
teste_regressao <- claims_sem_outliers[indices_amostra[(tamanho_treinamento + 1):nrow(claims_sem_outliers)], ]
treino_regressao_normalizado <- scale(treino_regressao[c(1, 3, 4)])
#colnames(treino_normalizado)[colnames(treino_normalizado) == "valorv"] <- "valorv_norm"
#colnames(treino_normalizado)[colnames(treino_normalizado) == "csinistros"] <- "csinistros_norm"
#colnames(treino_normalizado)[colnames(treino_normalizado) == "cmsinistro"] <- "cmsinistro_norm"
treino_regressao = cbind(treino_regressao, treino_regressao_normalizado)
modelo_regressao <- lm(cmsinistro ~ valorv + areacE + csinistros, data = treino_regressao)
summary(modelo)
modelo_regressao <- lm(cmsinistro ~ valorv + areacE + csinistros, data = treino_regressao)
summary(modelo_regressao)
# Faça previsões com o modelo nos dados de teste
previsoes <- predict(modelo_regressao, newdata = teste_regressao)
# Calculando o RMSE
rmse_regressao <- rmse(previsoes, teste_regressao$cmsinistro)
cat("RMSE:", rmse_regressao)
residuos <- residuals(modelo_regressao)
plot(modelo_regressao, which = 1)
residuos <- residuals(modelo_regressao)
plot(modelo_regressao, which = 1)
#install.packages("ggplot2")
#install.packages("dplyr")
#install.packages("caret")
#install.packages("corrplot")
#install.packages("Metrics")
#install.packages("rpart")
#install.packages("rpart.plot")
library(ggplot2)
library(dplyr)
library(rmarkdown)
library(caret)
library(corrplot)
library(Metrics)
library(rpart)
library(rpart.plot)
claims <- read.csv2("claims.csv")
#Criando a variavel target cmsinistro
claims <- claims %>% mutate(cmsinistro = csinistros / nsinistros)
head(claims)
#Dimensão do dataframe
dim(claims)
#Nome das colunas (atributos)
names(claims)
#Verificando se existem dados faltantes
sum(is.na(claims))
#exibindo primeiros registros
head(claims)
# Analise por tipo de veiculo
claims %>% ggplot() + geom_point(aes(x = tipov, y = cmsinistro))
claims %>% ggplot() + geom_boxplot(aes(x = tipov, y = cmsinistro))
#Analise por idade do condutor
claims %>% ggplot() + geom_point(aes(x = idadec, y = cmsinistro))
claims %>% ggplot() + geom_boxplot(aes(x = idadec, y = cmsinistro, group = idadec))
#Analise por idade do veiculo
claims %>% ggplot() + geom_point(aes(x = idadev, y = cmsinistro))
claims %>% ggplot() + geom_boxplot(aes(x = idadev, y = cmsinistro, group = idadev))
#Analise por sexo
claims %>% ggplot() + geom_point(aes(x = sexoc, y = cmsinistro))
claims %>% ggplot() + geom_boxplot(aes(x = sexoc, y = cmsinistro))
#Analise por area
claims %>% ggplot() + geom_point(aes(x = areac, y = cmsinistro))
claims %>% ggplot() + geom_boxplot(aes(x = areac, y = cmsinistro))
dummies_vars = dummyVars(~tipov + sexoc + areac, data = claims)
dummies_dados <- predict(dummies_vars, newdata = claims)
claims_dummies = cbind(claims, dummies_dados)
claims_dummies <- dplyr::select(claims_dummies, -c(tipov, sexoc, areac))
#matriz de correlação
correlacao_dummies <- cor(claims_dummies)
ggplot(data = reshape2::melt(correlacao_dummies)) +
geom_tile(aes(Var1, Var2, fill = value)) +
scale_fill_gradient(low = "blue", high = "white") +
theme_minimal() +
labs(title = "Mapa de Correlação Linear") +
theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))
as.data.frame(correlacao_dummies) %>% arrange(cmsinistro)
correlacao_forte <- correlacao_dummies[correlacao_dummies[,7] > 0.9, ]
#expos
#areacE
#csinistros
ggplot(data = reshape2::melt(correlacao_forte)) +
geom_tile(aes(Var1, Var2, fill = value)) +
scale_fill_gradient(low = "blue", high = "white") +
theme_minimal() +
labs(title = "Mapa de Correlação Linear") +
theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))
head(claims_dummies)
claims_features <- claims_dummies[c(1, 25, 4, 7)]
claims_features %>% ggplot() + geom_boxplot(aes(y = cmsinistro))
ggplot(data = claims_features, aes(y = cmsinistro)) +
geom_boxplot(fill = "lightblue") +
labs(title = "Boxplot da Sétima Coluna", y = "Valores")
# Calcular o primeiro quartil (Q1) e o terceiro quartil (Q3)
q1 <- quantile(claims_features$cmsinistro, probs = 0.25)
q3 <- quantile(claims_features$cmsinistro, 0.75)
# Calcular o intervalo interquartil (IQR)
iqr <- q3 - q1
# Definir os limites para identificar outliers
limite_inferior <- q1 - 1.5 * iqr
limite_superior <- q3 + 1.5 * iqr
# Remover outliers com base nos limites
claims_sem_outliers <- subset(claims_features, cmsinistro >= limite_superior)
# Definir a proporção desejada de treinamento e teste (por exemplo, 80% de treinamento, 20% de teste)
proporcao_treinamento <- 0.8
# Calcular o tamanho dos conjuntos de treinamento e teste com base na proporção
tamanho_treinamento <- round(nrow(claims_sem_outliers) * proporcao_treinamento)
tamanho_teste <- nrow(claims_sem_outliers) - tamanho_treinamento
# Criar índices aleatórios para amostrar os dados
indices_amostra <- sample(1:nrow(claims_sem_outliers))
# Dividir o dataframe em conjuntos de treinamento e teste
treino_regressao <- claims_sem_outliers[indices_amostra[1:tamanho_treinamento], ]
teste_regressao <- claims_sem_outliers[indices_amostra[(tamanho_treinamento + 1):nrow(claims_sem_outliers)], ]
treino_regressao_normalizado <- scale(treino_regressao[c(1, 3, 4)])
#colnames(treino_normalizado)[colnames(treino_normalizado) == "valorv"] <- "valorv_norm"
#colnames(treino_normalizado)[colnames(treino_normalizado) == "csinistros"] <- "csinistros_norm"
#colnames(treino_normalizado)[colnames(treino_normalizado) == "cmsinistro"] <- "cmsinistro_norm"
treino_regressao = cbind(treino_regressao, treino_regressao_normalizado)
modelo_regressao <- lm(cmsinistro ~ valorv + areacE + csinistros, data = treino_regressao)
summary(modelo_regressao)
# Faça previsões com o modelo nos dados de teste
previsoes <- predict(modelo_regressao, newdata = teste_regressao)
# Calculando o RMSE
rmse_regressao <- rmse(previsoes, teste_regressao$cmsinistro)
cat("RMSE:", rmse_regressao)
residuos <- residuals(modelo_regressao)
plot(modelo_regressao, which = 1)
# Definir a proporção desejada de treinamento e teste (por exemplo, 80% de treinamento, 20% de teste)
proporcao_treinamento <- 0.8
# Calcular o tamanho dos conjuntos de treinamento e teste com base na proporção
tamanho_treinamento <- round(nrow(claims_features) * proporcao_treinamento)
tamanho_teste <- nrow(claims_features) - tamanho_treinamento
# Criar índices aleatórios para amostrar os dados
indices_amostra <- sample(1:nrow(claims_features))
# Dividir o dataframe em conjuntos de treinamento e teste
treino_arvore <- claims_features[indices_amostra[1:tamanho_treinamento], ]
teste_arvore <- claims_features[indices_amostra[(tamanho_treinamento + 1):nrow(claims_features)], ]
# Ajustar o modelo da árvore de decisão
modelo_arvore <- rpart(cmsinistro ~ valorv + areacE + csinistros, data = treino_arvore)
# Fazer previsões para novos dados
previsao <- predict(modelo_arvore, teste_arvore)
# Calcule o RMSE
rmse_arvore <- rmse(previsao, teste_arvore$cmsinistro)
# Imprima o RMSE
cat("RMSE Regressao:", rmse_regressao, "/ RMSE Arvore:", rmse_arvore)
claims <- read.csv2("data-science-puzzle (1)\train.csv")
train <- read.csv2("data-science-puzzle (1)\\train.csv")
train
train <- read.csv2("data-science-puzzle (1)\\train.csv", sep = ",")
claims
train
train <- train[, -"X023c68873b"]
train["X023c68873b"]
train[, -train$X023c68873b]
train[, -train$X023c68873b]
train[, train$X023c68873b]
train[train$X023c68873b]
train
train
train[, -2]
train
train[, -which(names(train) == "X023c68873b")]
train[, -which(names(train) == "X023c68873b" | names(train) == "id")]
train <- train[, -which(names(train) == "X023c68873b" | names(train) == "id")]
head(train)
summary(train)
sapply(seu_dataset, class)
sapply(train, class)
tipos_de_coluna_df <- data.frame(Coluna = names(train), Tipo = sapply(train, class))
print(tipos_de_coluna_df)
print(tipos_de_coluna_df[, 3])
print(tipos_de_coluna_df[1])
print(tipos_de_coluna_df[3])
print(tipos_de_coluna_df[2])
print(tipos_de_coluna_df[2 == "integer"])
print(tipos_de_coluna_df[tipos_de_coluna_df[2 == "integer"]])
tipos_de_coluna_df[tipos_de_coluna_df$Tipo == "character"]
names(tipos_de_coluna_df)
tipos_de_coluna_df
tipos_de_coluna_df[tipos_de_coluna_df[1] == "character"]
tipos_de_coluna_df[tipos_de_coluna_df[2] == "character"]
as.data.frame(tipos_de_coluna_df[tipos_de_coluna_df[2] == "character"])
tipos_de_coluna_df
tipos_de_coluna_df <- data.frame(Coluna = names(train), Tipo = sapply(train, class))
tipos_de_coluna_df
tipos_de_coluna_df %>% filter(is.character(Tipo))
library(ggplot2)
library(dplyr)
library(rmarkdown)
library(caret)
library(caret)
library(corrplot)
library(Metrics)
library(rpart)
library(rpart.plot)
tipos_de_coluna_df %>% filter(is.character(Tipo))
tipos_de_coluna_df %>% filter(is.character(Tipo))
tipos_de_coluna_df %>% filter(Tipo == "character")
train
train %>% select_if(is.character())
train %>% select_if(is.character)
train %>% select_if(is.character) %>% mutate_all(as.numeric)
head(train)
train <- read.csv2("data-science-puzzle (1)\\train.csv", sep = ",", dec = ".")
train <- read.csv2("data-science-puzzle (1)\\train.csv", sep = ",", dec = ".")
train
train <- train[, -which(names(train) == "X023c68873b" | names(train) == "id")]
head(train)
head(train)
train %>% select_if(is.character)
train <- read.csv2("data-science-puzzle (1)\\train.csv", sep = ",", dec = ".")
train_numericos <- train %>% select_if(is.numeric)
head(train_numericos)
head(train_numericos)
train_numericos <- train_numericos[, names(train) == "id")]
train_numericos <- train_numericos[, (names(train) == "id")]
train_numericos <- train_numericos[, -which(names(train) == "id")]
head(train_numericos)
train_numericos <- train_numericos[, -which(names(train_numericos) == "id")]
head(train_numericos)
sum(is.na(train_numericos))
train_numericos %>% ggplot() + geom_boxplot(aes(y = target))
# Calcular o primeiro quartil (Q1) e o terceiro quartil (Q3)
q1 <- quantile(train_numericos$target, probs = 0.25)
q3 <- quantile(train_numericos$target, 0.75)
# Calcular o intervalo interquartil (IQR)
iqr <- q3 - q1
# Definir os limites para identificar outliers
limite_inferior <- q1 - 1.5 * iqr
limite_superior <- q3 + 1.5 * iqr
# Remover outliers com base nos limites
train_sem_outliers <- subset(train_numericos, target >= limite_superior)
train_sem_outliers %>% ggplot() + geom_boxplot(aes(y = target))
# Calcular o primeiro quartil (Q1) e o terceiro quartil (Q3)
q1 <- quantile(train_numericos$target, probs = 0.25)
q3 <- quantile(train_numericos$target, 0.75)
# Calcular o intervalo interquartil (IQR)
iqr <- q3 - q1
iqr
# Definir os limites para identificar outliers
limite_inferior <- q1 - 1.5 * iqr
limite_superior <- q3 + 1.5 * iqr
limite_superior
# Remover outliers com base nos limites
train_sem_outliers <- subset(train_numericos, target <= limite_superior)
train_sem_outliers %>% ggplot() + geom_boxplot(aes(y = target))
train_sem_outliers
train_numericos
train_sem_outliers %>% ggplot() + geom_boxplot(aes(y = target))
#matriz de correlação
correlacao <- cor(train_sem_outliers)
correlacao
ggplot(data = reshape2::melt(correlacao)) +
geom_tile(aes(Var1, Var2, fill = value)) +
scale_fill_gradient(low = "blue", high = "white") +
theme_minimal() +
labs(title = "Mapa de Correlação Linear") +
theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))
ggplot(data = reshape2::melt(correlacao)) +
geom_tile(aes(Var1, Var2, fill = value)) +
scale_fill_gradient(low = "blue", high = "white") +
theme_minimal() +
labs(title = "Mapa de Correlação Linear") +
theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1), width = 100)
ggplot(data = reshape2::melt(correlacao)) +
geom_tile(aes(Var1, Var2, fill = value)) +
scale_fill_gradient(low = "blue", high = "white") +
theme_minimal() +
labs(title = "Mapa de Correlação Linear") +
theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1), plot.width = 10)
ggplot(data = reshape2::melt(correlacao)) +
geom_tile(aes(Var1, Var2, fill = value)) +
scale_fill_gradient(low = "blue", high = "white") +
theme_minimal() +
labs(title = "Mapa de Correlação Linear") +
theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))
# Defina o tamanho desejado em polegadas
largura <- 10  # Largura em polegadas
altura <- 8    # Altura em polegadas
# Salve o gráfico com as dimensões especificadas
ggsave("mapa_correlacao.png", plot = meu_grafico, width = largura, height = altura)
meu_grafico  <- ggplot(data = reshape2::melt(correlacao)) +
geom_tile(aes(Var1, Var2, fill = value)) +
scale_fill_gradient(low = "blue", high = "white") +
theme_minimal() +
labs(title = "Mapa de Correlação Linear") +
theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))
# Defina o tamanho desejado em polegadas
largura <- 10  # Largura em polegadas
altura <- 8    # Altura em polegadas
# Salve o gráfico com as dimensões especificadas
ggsave("mapa_correlacao.png", plot = meu_grafico, width = largura, height = altura)
altura <- 15    # Altura em polegadas
# Salve o gráfico com as dimensões especificadas
ggsave("mapa_correlacao.png", plot = meu_grafico, width = largura, height = altura)
# Defina o tamanho desejado em polegadas
largura <- 20  # Largura em polegadas
altura <- 15    # Altura em polegadas
# Salve o gráfico com as dimensões especificadas
ggsave("mapa_correlacao.png", plot = meu_grafico, width = largura, height = altura)
correlacao
correlacao[correlacao[,"target"] > 0.4, ]
names(correlacao)
ncol(correlacao)
correlacao[correlacao[,ncol(correlacao)] > 0.4, ]
correlacao[,ncol(correlacao)]]
correlacao[,ncol(correlacao)]
train_normalizado <- scale(train_sem_outliers[,-100])
train_normalizado
as.data.frame(train_normalizado)
train_sem_outliers
library(stats)
library(prcomp)
#install.packages("ggplot2")
#install.packages("dplyr")
#install.packages("caret")
#install.packages("corrplot")
#install.packages("Metrics")
#install.packages("rpart")
#install.packages("rpart.plot")
install.packages("stats")
install.packages("prcomp")
library(stats)
library(prcomp)
pca_result <- prcomp(train_sem_outliers[, -100], scale = TRUE)
pca_result
summary(pca_result)
pca_result$rotation
pca_result$sdev^2
summary(pca_result)
pca_result$sdev^2
pca_result$sdev^2 / sum(pca_result$sdev^2)
pca_result$sdev
pca_result$sdev^2 / sum(pca_result$sdev^2)
categorical_to_numeric = function (x) {
# Convert a categorical vector x to a numeric one
return(as.numeric(factor(x, labels=(1:length(levels(factor(x)))))))
}
find_model = function (x, target, predictors, base=NULL) {
# Find the best multiple linear regression model using backwards
# elimination. Features with highest p-value (low signficante) are
# eliminated until find the model with the highest Adjusted R-squared
model = lm(formula=lm_formula(target, predictors), data=x)
model$summary = summary(model)
if (is.null(base))
base = model
if (model$summary$adj.r.squared >= base$summary$adj.r.squared) {
p.values = model$summary$coefficients[predictors, 'Pr(>|t|)']
worst_predictor = names(sort(p.values, decreasing=TRUE)[1])
find_model(x, target, setdiff(predictors, worst_predictor), model)
} else {
return(base)
}
}
lm_formula = function (target, predictors) {
# Create a linear regression formula dynamically
return(as.formula(sprintf('%s ~%s', target, paste(predictors, collapse='+'))))
}
standardize = function (x) {
# Normalize a numeric vector x using its z-score = (x-mean(x))/sd(x)
# If x is categorical, it will be converted into a numeric one
if (is.numeric(x))
return(scale(x))
else
return(standardize(categorical_to_numeric(x)))
}
unstandardize = function (y, x) {
# Un-standardize a vector y using a vector x as reference
return(mean(x) + (y * sd(x)))
}
main = function () {
# Load and standardize training data set
training = read.csv('train.csv')
training.target = training$target  # labels backup
training_features = setdiff(colnames(training), union('id', 'target'))
training = data.frame(cbind(
apply(training[, union(training_features, 'target')], 2, standardize),
id=training$id
))
# Load and standardize testing data set
testing = read.csv('test.csv')
testing_features = setdiff(colnames(testing), c('id'))
testing = data.frame(cbind(
apply(testing[, testing_features], 2, standardize),
id=testing$id
))
# Full linear model, all 106 training features are considered
# Adjusted R-squared: 0.2586891
# p-value: < 2.2e-16
# summary(lm(formula=lm_formula('target', training_features), training))
# Find the best linear model, i.e. with the highest Adjusted R-squared
# Only 70 training features are considered in the model
# Adjusted R-squared: 0.2593609
# p-value: < 2.2e-16
model = find_model(training, 'target', training_features)
print(model$summary)
# Predict and un-standardize targets of testing data
prediction = unstandardize(predict(model, testing), training.target)
# Save CSV file
output = cbind(id=training$id, prediction=prediction)
write.csv(output, 'output.csv', row.names=FALSE)
}
system.time(main())
componente_principal_i <- 3  # Substitua pelo número do componente desejado
prop_var_cumulativa_i <- sum(prop_var_exp[1:componente_principal_i])
prop_var_cumulativa_i <- sum(pca_result[1:componente_principal_i])
pca_result$sdev
prop_var_exp <- pca_result$sdev^2 / sum(pca_result$sdev^2)
componente_principal_i <- 3  # Substitua pelo número do componente desejado
prop_var_cumulativa_i <- sum(prop_var_exp[1:componente_principal_i])
prop_var_cumulativa_i
componente_principal_i <- 10  # Substitua pelo número do componente desejado
prop_var_cumulativa_i <- sum(prop_var_exp[1:componente_principal_i])
prop_var_cumulativa_i
componente_principal_i <- 30  # Substitua pelo número do componente desejado
prop_var_cumulativa_i <- sum(prop_var_exp[1:componente_principal_i])
prop_var_cumulativa_i
componente_principal_i <- 50  # Substitua pelo número do componente desejado
prop_var_cumulativa_i <- sum(prop_var_exp[1:componente_principal_i])
prop_var_cumulativa_i
componente_principal_i <- 55  # Substitua pelo número do componente desejado
prop_var_cumulativa_i <- sum(prop_var_exp[1:componente_principal_i])
prop_var_cumulativa_i
# Determinar o número de componentes principais a serem retidos com base no limiar da variância cumulativa
limiar_var_cumulativa <- 0.72  # Substitua pelo seu limiar desejado
num_componentes_a_reter <- sum(cumsum(prop_var_exp) < limiar_var_cumulativa) + 1
# Refazer a PCA com o número selecionado de componentes principais
pca_result_reduzido <- prcomp
pca_result_reduzido
# Refazer a PCA com o número selecionado de componentes principais
pca_result_reduzido <- prcomp(train_sem_outliers, scale. = TRUE, n.comp = num_componentes_a_reter)
pca_result_reduzido
train <- read.csv2("data-science-puzzle (1)\\train.csv", sep = ",", dec = ".")
train <- read.csv2("data-science-puzzle (1)\\train.csv", sep = ",", dec = ".")
train_numericos <- train %>% select_if(is.character)
train_normalizado <- scale(train_numericos)
train <- read.csv2("data-science-puzzle (1)\\train.csv", sep = ",", dec = ".")
train <- read.csv2("data-science-puzzle (1)\\train.csv", sep = ",", dec = ".")
train_numericos <- train %>% select_if(is.character)
# Perform forward selection
forward_model <- lm(y ~ 1, data = train_normalizado)  # Start with an intercept-only model
train_normalizado <- as.data.frame(scale(train_numericos))
train_normalizado
# Perform forward selection
forward_model <- lm(y ~ 1, data = train_normalizado)  # Start with an intercept-only model
train_normalizado
train <- read.csv2("data-science-puzzle (1)\\train.csv", sep = ",", dec = ".")
train <- read.csv2("data-science-puzzle (1)\\train.csv", sep = ",", dec = ".")
train_numericos <- train %>% select_if(is.character)
train <- read.csv2("data-science-puzzle (1)\\train.csv", sep = ",", dec = ".")
train_numericos <- train %>% select_if(is.character)
library(dplyr)
train_numericos <- train %>% select_if(is.character)
train_numericos <- train_numericos[, -which(names(train_numericos) == "id")]
train_numericos
library(dplyr)
train <- read.csv2("data-science-puzzle (1)\\train.csv", sep = ",", dec = ".")
train <- read.csv2("data-science-puzzle (1)\\train.csv", sep = ",", dec = ".")
train %>% select_if(is.character)
train
train <- train_numericos[, -which(names(train_numericos) == "X023c68873b" | names(train_numericos) == "id")]
train
